{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib   \n",
    "import os   \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, AutoModel\n",
    "\n",
    "from Preprocessing import preprocess_text\n",
    "\n",
    "# os.chdir('C:/Users/LENOVO/GitHub/Jurnal-Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = outputs['last_hidden_state'][:, 0, :]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Buat dataloader\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, tokenizer, max_length=256):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=max_length,\n",
    "                            pad_to_max_length=True,\n",
    "                            return_attention_mask=True,\n",
    "                            return_tensors='pt'\n",
    "                       )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/extracted_publication_journal_s2.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertClassifier(12)\n",
    "model.load_state_dict(torch.load('model/finetuned_model_sinta_translated.pt'))\n",
    "\n",
    "# Membuat dataloader\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jour in df.journal.unique():\n",
    "\n",
    "    journal_type = 'sinta_new_data_s2'\n",
    "\n",
    "    # Assuming 'jurnal_id' is a variable containing the directory name\n",
    "    file_path = os.path.join('src', journal_type, jour)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.mkdir(file_path)\n",
    "\n",
    "        data = df[df['journal'] == jour]\n",
    "        data['data_cleaned'] = data['title'] + data['abstract'] \n",
    "        data['data_cleaned'] = data['data_cleaned'].apply(lambda x : preprocess_text(x))\n",
    "        X = list(data['data_cleaned'])\n",
    "\n",
    "        input_ids, attention_masks = tokenize_data(X, tokenizer)\n",
    "        dataset = ArticleDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        device = 'cpu'\n",
    "        if torch.cuda.is_available() :\n",
    "            device = 'cuda'\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Set model ke mode evaluasi (non-training)\n",
    "        model.eval()\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                embeddings.append(outputs.cpu().numpy())\n",
    "\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "        # Mengubah array embeddings menjadi matriks dua dimensi\n",
    "        X = embeddings.reshape(embeddings.shape[0], -1)\n",
    "\n",
    "        pca = PCA(n_components=2, random_state=0)\n",
    "        X = pca.fit_transform(X)\n",
    "\n",
    "        # Perform KMeans clustering\n",
    "        num_clusters = 1\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=0, max_iter=1000)\n",
    "        kmeans.fit(X)\n",
    "\n",
    "        # Assign each journal to its cluster\n",
    "        cluster_labels = kmeans.labels_\n",
    "\n",
    "        # Mendapatkan koordinat pusat cluster\n",
    "        centroid = kmeans.cluster_centers_\n",
    "\n",
    "        # Menghitung jarak antara setiap titik data dengan centroid\n",
    "        jarak_ke_centroid = np.sqrt(np.sum((X - centroid)**2, axis=1))\n",
    "\n",
    "        # Menentukan batas jarak yang dianggap sebagai \"outscoop\"\n",
    "        outscoop_threshold = np.mean(jarak_ke_centroid) + 2 * np.std(jarak_ke_centroid)\n",
    "\n",
    "        # Memisahkan data yang masih masuk dalam \"scoop\" dan \"outscoop\"\n",
    "        scoop_data = X[jarak_ke_centroid <= outscoop_threshold]\n",
    "        outscoop_data = X[jarak_ke_centroid > outscoop_threshold]\n",
    "\n",
    "        scoop_labels = np.ones(len(X))\n",
    "        scoop_labels[jarak_ke_centroid > outscoop_threshold] = -1\n",
    "\n",
    "        filename_kmeans = f\"{file_path}/{jour}_kmeans.pkl\"\n",
    "        joblib.dump(kmeans, filename_kmeans)\n",
    "\n",
    "        np.save(f\"{file_path}/{jour}_threshold.npy\", outscoop_threshold)\n",
    "        np.save(f\"{file_path}/{jour}_pca_data.npy\", X)\n",
    "        np.save(f\"{file_path}/{jour}_bert_data.npy\", embeddings.reshape(embeddings.shape[0], -1))\n",
    "\n",
    "        df_res = pd.DataFrame({'Data': data['data_cleaned'],\n",
    "                   'Label': scoop_labels})\n",
    "\n",
    "        inScoop_df = df_res[df_res['Label'] == 1]\n",
    "        outScoop_df = df_res[df_res['Label'] == -1]\n",
    "\n",
    "        df_res.to_csv(f'{file_path}/{jour}_data_jurnal.csv')\n",
    "        inScoop_df.to_csv(f'{file_path}/{jour}_inscoop_data_jurnal.csv')\n",
    "        outScoop_df.to_csv(f'{file_path}/{jour}_outscoop_data_jurnal.csv')\n",
    "\n",
    "        print(\"Data sebaran PCA {} telah disimpan.\".format(jour))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = df[df['journal'] == '25024760']\n",
    "# data['data_cleaned'] = data['title'] + data['abstract'] \n",
    "# data['data_cleaned'].apply(lambda x : preprocess_text(x))\n",
    "# X = list(data['data_cleaned'])\n",
    "\n",
    "# input_ids, attention_masks = tokenize_data(X, tokenizer)\n",
    "# dataset = ArticleDataset(input_ids, attention_masks)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "# if torch.cuda.is_available() :\n",
    "#     device = 'cuda'\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# # Set model ke mode evaluasi (non-training)\n",
    "# model.eval()\n",
    "\n",
    "# # Embedding\n",
    "# embeddings = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in dataloader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#         embeddings.append(outputs.cpu().numpy())\n",
    "\n",
    "# embeddings = np.concatenate(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Mengubah array embeddings menjadi matriks dua dimensi\n",
    "# X = embeddings.reshape(embeddings.shape[0], -1)\n",
    "\n",
    "# pca = PCA(n_components=2, random_state=0)\n",
    "# X = pca.fit_transform(X)\n",
    "\n",
    "# # Perform KMeans clustering\n",
    "# num_clusters = 1\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=0, max_iter=1000)\n",
    "# kmeans.fit(X)\n",
    "\n",
    "# # Assign each journal to its cluster\n",
    "# cluster_labels = kmeans.labels_\n",
    "\n",
    "# # Mendapatkan koordinat pusat cluster\n",
    "# centroid = kmeans.cluster_centers_\n",
    "\n",
    "# # Menghitung jarak antara setiap titik data dengan centroid\n",
    "# jarak_ke_centroid = np.sqrt(np.sum((X - centroid)**2, axis=1))\n",
    "\n",
    "# # Menentukan batas jarak yang dianggap sebagai \"outscoop\"\n",
    "# outscoop_threshold = np.mean(jarak_ke_centroid) + 2 * np.std(jarak_ke_centroid)\n",
    "\n",
    "# # Memisahkan data yang masih masuk dalam \"scoop\" dan \"outscoop\"\n",
    "# scoop_data = X[jarak_ke_centroid <= outscoop_threshold]\n",
    "# outscoop_data = X[jarak_ke_centroid > outscoop_threshold]\n",
    "\n",
    "# scoop_labels = np.ones(len(X))\n",
    "# scoop_labels[jarak_ke_centroid > outscoop_threshold] = -1\n",
    "\n",
    "# filename_kmeans = f\"{file_path}/{jour}_kmeans.pkl\"\n",
    "# joblib.dump(kmeans, filename_kmeans)\n",
    "\n",
    "# np.save(f\"{file_path}/{jour}_threshold.npy\", outscoop_threshold)\n",
    "# np.save(f\"{file_path}/{jour}_pca_data.npy\", X)\n",
    "# np.save(f\"{file_path}/{jour}_bert_data.npy\", embeddings.reshape(embeddings.shape[0], -1))\n",
    "\n",
    "# df_res = pd.DataFrame({'Data': data['data_cleaned'],\n",
    "#             'Label': scoop_labels})\n",
    "\n",
    "# inScoop_df = df_res[df_res['Label'] == 1]\n",
    "# outScoop_df = df_res[df_res['Label'] == -1]\n",
    "\n",
    "# df_res.to_csv(f'{file_path}/{jour}_data_jurnal.csv')\n",
    "# inScoop_df.to_csv(f'{file_path}/{jour}_inscoop_data_jurnal.csv')\n",
    "# outScoop_df.to_csv(f'{file_path}/{jour}_outscoop_data_jurnal.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
